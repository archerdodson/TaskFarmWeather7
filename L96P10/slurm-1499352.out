/scrtp/avon/eb/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Train SR network for lorenz96 model with lr 0.03 using SignatureKernel scoring rule
  0%|          | 0/1000 [00:00<?, ?it/s]/gpfs/home/stats/stubxk/ArcherDissEnv3/lib/python3.11/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 9 will likely result in GPU under-utilization due to low occupancy.[0m
  warn(NumbaPerformanceWarning(msg))
/gpfs/home/stats/stubxk/ArcherDissEnv3/lib/python3.11/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: [1mGrid size 3 will likely result in GPU under-utilization due to low occupancy.[0m
  warn(NumbaPerformanceWarning(msg))
  0%|          | 1/1000 [05:42<94:57:34, 342.20s/it]  0%|          | 2/1000 [11:23<94:40:31, 341.51s/it]  0%|          | 3/1000 [17:04<94:30:55, 341.28s/it]  0%|          | 4/1000 [22:45<94:23:07, 341.15s/it]  0%|          | 5/1000 [28:26<94:16:11, 341.08s/it]  1%|          | 6/1000 [34:07<94:10:13, 341.06s/it]  1%|          | 7/1000 [39:51<94:22:23, 342.14s/it]  1%|          | 8/1000 [45:34<94:23:09, 342.53s/it]  1%|          | 9/1000 [51:14<94:00:54, 341.53s/it]  1%|          | 10/1000 [56:51<93:33:44, 340.23s/it]  1%|          | 11/1000 [1:02:25<92:57:37, 338.38s/it]  1%|          | 12/1000 [1:08:00<92:35:36, 337.38s/it]  1%|â–         | 13/1000 [1:13:36<92:19:14, 336.73s/it]  1%|â–         | 14/1000 [1:19:11<92:05:49, 336.26s/it]  2%|â–         | 15/1000 [1:24:46<91:54:57, 335.94s/it]  2%|â–         | 16/1000 [1:30:21<91:45:15, 335.69s/it]  2%|â–         | 17/1000 [1:35:56<91:37:03, 335.53s/it]  2%|â–         | 18/1000 [1:41:32<91:30:54, 335.49s/it]  2%|â–         | 19/1000 [1:47:07<91:24:28, 335.44s/it]  2%|â–         | 20/1000 [1:52:42<91:17:47, 335.37s/it]  2%|â–         | 21/1000 [1:58:17<91:11:32, 335.33s/it]  2%|â–         | 22/1000 [2:03:53<91:06:22, 335.36s/it]  2%|â–         | 23/1000 [2:09:30<91:10:27, 335.95s/it]  2%|â–         | 24/1000 [2:15:07<91:10:33, 336.30s/it]  2%|â–Ž         | 25/1000 [2:20:44<91:09:00, 336.55s/it]  3%|â–Ž         | 26/1000 [2:26:21<91:05:39, 336.69s/it]  3%|â–Ž         | 27/1000 [2:31:58<91:01:14, 336.77s/it]  3%|â–Ž         | 28/1000 [2:37:35<90:57:12, 336.86s/it]  3%|â–Ž         | 29/1000 [2:43:12<90:51:30, 336.86s/it]  3%|â–Ž         | 30/1000 [2:48:49<90:46:25, 336.89s/it]  3%|â–Ž         | 31/1000 [2:54:26<90:41:02, 336.91s/it]  3%|â–Ž         | 32/1000 [3:00:03<90:35:37, 336.92s/it]  3%|â–Ž         | 33/1000 [3:05:40<90:31:17, 337.00s/it]  3%|â–Ž         | 34/1000 [3:11:17<90:25:06, 336.96s/it]  4%|â–Ž         | 35/1000 [3:16:54<90:19:10, 336.94s/it]  4%|â–Ž         | 36/1000 [3:22:31<90:13:05, 336.91s/it]  4%|â–Ž         | 37/1000 [3:28:08<90:07:26, 336.91s/it]  4%|â–         | 38/1000 [3:33:45<90:02:39, 336.96s/it]  4%|â–         | 39/1000 [3:39:22<89:55:37, 336.88s/it]  4%|â–         | 40/1000 [3:44:58<89:48:16, 336.77s/it]  4%|â–         | 41/1000 [3:50:35<89:42:37, 336.76s/it]  4%|â–         | 42/1000 [3:56:12<89:37:06, 336.77s/it]  4%|â–         | 43/1000 [4:01:49<89:33:07, 336.87s/it]  4%|â–         | 44/1000 [4:07:26<89:27:47, 336.89s/it]  4%|â–         | 45/1000 [4:13:03<89:22:00, 336.88s/it]  5%|â–         | 46/1000 [4:18:40<89:16:45, 336.90s/it]  5%|â–         | 47/1000 [4:24:16<89:11:11, 336.91s/it]  5%|â–         | 48/1000 [4:29:54<89:07:15, 337.01s/it]  5%|â–         | 49/1000 [4:35:31<89:01:38, 337.01s/it]  5%|â–Œ         | 50/1000 [4:41:08<88:55:28, 336.98s/it]  5%|â–Œ         | 51/1000 [4:46:45<88:49:32, 336.96s/it]  5%|â–Œ         | 52/1000 [4:52:22<88:44:10, 336.97s/it]  5%|â–Œ         | 53/1000 [4:57:59<88:39:08, 337.01s/it]  5%|â–Œ         | 54/1000 [5:03:36<88:33:35, 337.01s/it]  6%|â–Œ         | 55/1000 [5:09:12<88:27:07, 336.96s/it]  6%|â–Œ         | 56/1000 [5:14:49<88:21:12, 336.94s/it]  6%|â–Œ         | 57/1000 [5:20:26<88:16:10, 336.98s/it]  6%|â–Œ         | 58/1000 [5:26:04<88:11:34, 337.04s/it]  6%|â–Œ         | 59/1000 [5:31:41<88:06:50, 337.10s/it]  6%|â–Œ         | 60/1000 [5:37:18<88:00:48, 337.07s/it]  6%|â–Œ         | 61/1000 [5:42:55<87:54:23, 337.02s/it]  6%|â–Œ         | 62/1000 [5:48:32<87:48:10, 336.98s/it]  6%|â–‹         | 63/1000 [5:54:09<87:43:28, 337.04s/it]  6%|â–‹         | 64/1000 [5:59:46<87:37:50, 337.04s/it]  6%|â–‹         | 65/1000 [6:05:23<87:31:32, 337.00s/it]  7%|â–‹         | 66/1000 [6:11:00<87:25:46, 336.99s/it]  7%|â–‹         | 67/1000 [6:16:37<87:20:35, 337.02s/it]  7%|â–‹         | 68/1000 [6:22:14<87:13:56, 336.95s/it]  7%|â–‹         | 69/1000 [6:27:50<87:07:09, 336.87s/it]  7%|â–‹         | 70/1000 [6:33:27<87:00:52, 336.83s/it]  7%|â–‹         | 71/1000 [6:39:04<86:54:51, 336.80s/it]  7%|â–‹         | 72/1000 [6:Epoch: 1/1000. Train set: Average loss: 1.0282
Epoch: 1/1000. Validation set: Average loss: -0.6719
Epoch: 2/1000. Train set: Average loss: -0.6821
Epoch: 2/1000. Validation set: Average loss: -0.9353
Epoch: 3/1000. Train set: Average loss: -1.1000
Epoch: 3/1000. Validation set: Average loss: -1.1072
Epoch: 4/1000. Train set: Average loss: -1.6042
Epoch: 4/1000. Validation set: Average loss: -2.1687
Epoch: 5/1000. Train set: Average loss: -2.2219
Epoch: 5/1000. Validation set: Average loss: -2.0276
Epoch: 6/1000. Train set: Average loss: -2.1187
Epoch: 6/1000. Validation set: Average loss: -2.1943
Epoch: 7/1000. Train set: Average loss: -2.2179
Epoch: 7/1000. Validation set: Average loss: -2.2127
Epoch: 8/1000. Train set: Average loss: -2.2300
Epoch: 8/1000. Validation set: Average loss: -2.2462
Epoch: 9/1000. Train set: Average loss: -2.2488
Epoch: 9/1000. Validation set: Average loss: -2.2551
Epoch: 10/1000. Train set: Average loss: -2.2530
Epoch: 10/1000. Validation set: Average loss: -2.2515
Epoch: 11/1000. Train set: Average loss: -2.2573
Epoch: 11/1000. Validation set: Average loss: -2.2597
Epoch: 12/1000. Train set: Average loss: -2.2604
Epoch: 12/1000. Validation set: Average loss: -2.2696
Epoch: 13/1000. Train set: Average loss: -2.2702
Epoch: 13/1000. Validation set: Average loss: -2.2601
Epoch: 14/1000. Train set: Average loss: -2.2460
Epoch: 14/1000. Validation set: Average loss: -2.2046
Epoch: 15/1000. Train set: Average loss: -2.2155
Epoch: 15/1000. Validation set: Average loss: -2.2263
Epoch: 16/1000. Train set: Average loss: -2.2388
Epoch: 16/1000. Validation set: Average loss: -2.2477
Epoch: 17/1000. Train set: Average loss: -2.2505
Epoch: 17/1000. Validation set: Average loss: -2.2490
Epoch: 18/1000. Train set: Average loss: -2.2585
Epoch: 18/1000. Validation set: Average loss: -2.2549
Epoch: 19/1000. Train set: Average loss: -2.2614
Epoch: 19/1000. Validation set: Average loss: -2.2638
Epoch: 20/1000. Train set: Average loss: -2.2636
Epoch: 20/1000. Validation set: Average loss: -2.2660
Epoch: 21/1000. Train set: Average loss: -2.2677
Epoch: 21/1000. Validation set: Average loss: -2.2661
Epoch: 22/1000. Train set: Average loss: -2.2682
Epoch: 22/1000. Validation set: Average loss: -2.2677
Epoch: 23/1000. Train set: Average loss: -2.2711
Epoch: 23/1000. Validation set: Average loss: -2.2708
Epoch: 24/1000. Train set: Average loss: -2.2691
Epoch: 24/1000. Validation set: Average loss: -2.2662
Epoch: 25/1000. Train set: Average loss: -2.2673
Epoch: 25/1000. Validation set: Average loss: -2.2694
Epoch: 26/1000. Train set: Average loss: -2.2687
Epoch: 26/1000. Validation set: Average loss: -2.2724
Epoch: 27/1000. Train set: Average loss: -2.2708
Epoch: 27/1000. Validation set: Average loss: -2.2732
Epoch: 28/1000. Train set: Average loss: -2.2740
Epoch: 28/1000. Validation set: Average loss: -2.2737
Epoch: 29/1000. Train set: Average loss: -2.2761
Epoch: 29/1000. Validation set: Average loss: -2.2746
Epoch: 30/1000. Train set: Average loss: -2.2753
Epoch: 30/1000. Validation set: Average loss: -2.2744
Epoch: 31/1000. Train set: Average loss: -2.2642
Epoch: 31/1000. Validation set: Average loss: -2.2407
Epoch: 32/1000. Train set: Average loss: -2.1407
Epoch: 32/1000. Validation set: Average loss: -1.9410
Epoch: 33/1000. Train set: Average loss: -2.1276
Epoch: 33/1000. Validation set: Average loss: -2.2244
Epoch: 34/1000. Train set: Average loss: -2.2333
Epoch: 34/1000. Validation set: Average loss: -2.2374
Epoch: 35/1000. Train set: Average loss: -2.2463
Epoch: 35/1000. Validation set: Average loss: -2.2491
Epoch: 36/1000. Train set: Average loss: -2.2522
Epoch: 36/1000. Validation set: Average loss: -2.2595
Epoch: 37/1000. Train set: Average loss: -2.2553
Epoch: 37/1000. Validation set: Average loss: -2.2573
Epoch: 38/1000. Train set: Average loss: -2.2587
Epoch: 38/1000. Validation set: Average loss: -2.2560
Epoch: 39/1000. Train set: Average loss: -2.2637
Epoch: 39/1000. Validation set: Average loss: -2.2636
Epoch: 40/1000. Train set: Average loss: -2.2567
Epoch: 40/1000. Validation set: Average loss: -2.2634
Epoch: 41/1000. Train set: Average loss: -2.2646
Epoch: 41/1000. Validation set: Average loss: -2.2653
Epoch: 42/1000. Train set: Average loss: -2.2675
Epoch: 42/1000. Validation set: Average loss: -2.2670
Epoch: 43/1000. Train set: Average loss: -2.2643
Epoch: 43/1000. Validation set: Average loss: -2.2579
Epoch: 44/1000. Train set: Average loss: -2.2583
Epoch: 44/1000. Validation set: Average loss: -2.2563
Epoch: 45/1000. Train set: Average loss: -2.2613
Epoch: 45/1000. Validation set: Average loss: -2.2656
Epoch: 46/1000. Train set: Average loss: -2.2684
Epoch: 46/1000. Validation set: Average loss: -2.2677
Epoch: 47/1000. Train set: Average loss: -2.2728
Epoch: 47/1000. Validation set: Average loss: -2.2746
Epoch: 48/1000. Train set: Average loss: -2.2728
Epoch: 48/1000. Validation set: Average loss: -2.2670
Epoch: 49/1000. Train set: Average loss: -2.2721
Epoch: 49/1000. Validation set: Average loss: -2.2788
Epoch: 50/1000. Train set: Average loss: -2.2781
Epoch: 50/1000. Validation set: Average loss: -2.2759
Epoch: 51/1000. Train set: Average loss: -2.2784
Epoch: 51/1000. Validation set: Average loss: -2.2764
Epoch: 52/1000. Train set: Average loss: -2.2775
Epoch: 52/1000. Validation set: Average loss: -2.2786
Epoch: 53/1000. Train set: Average loss: -2.2777
Epoch: 53/1000. Validation set: Average loss: -2.2770
Epoch: 54/1000. Train set: Average loss: -2.2776
Epoch: 54/1000. Validation set: Average loss: -2.2777
Epoch: 55/1000. Train set: Average loss: -2.2772
Epoch: 55/1000. Validation set: Average loss: -2.2780
Epoch: 56/1000. Train set: Average loss: -2.2778
Epoch: 56/1000. Validation set: Average loss: -2.2780
Epoch: 57/1000. Train set: Average loss: -2.2776
Epoch: 57/1000. Validation set: Average loss: -2.2773
Epoch: 58/1000. Train set: Average loss: -2.2786
Epoch: 58/1000. Validation set: Average loss: -2.2789
Epoch: 59/1000. Train set: Average loss: -2.2782
Epoch: 59/1000. Validation set: Average loss: -2.2780
Epoch: 60/1000. Train set: Average loss: -2.2780
Epoch: 60/1000. Validation set: Average loss: -2.2787
Epoch: 61/1000. Train set: Average loss: -2.2786
Epoch: 61/1000. Validation set: Average loss: -2.2775
Epoch: 62/1000. Train set: Average loss: -2.2785
Epoch: 62/1000. Validation set: Average loss: -2.2786
Epoch: 63/1000. Train set: Average loss: -2.2779
Epoch: 63/1000. Validation set: Average loss: -2.2787
Epoch: 64/1000. Train set: Average loss: -2.2785
Epoch: 64/1000. Validation set: Average loss: -2.2779
Epoch: 65/1000. Train set: Average loss: -2.2787
Epoch: 65/1000. Validation set: Average loss: -2.2790
Epoch: 66/1000. Train set: Average loss: -2.2782
Epoch: 66/1000. Validation set: Average loss: -2.2784
Epoch: 67/1000. Train set: Average loss: -2.2779
Epoch: 67/1000. Validation set: Average loss: -2.2767
Epoch: 68/1000. Train set: Average loss: -2.2787
Epoch: 68/1000. Validation set: Average loss: -2.2784
Epoch: 69/1000. Train set: Average loss: -2.2791
Epoch: 69/1000. Validation set: Average loss: -2.2774
Epoch: 70/1000. Train set: Average loss: -2.2787
Epoch: 70/1000. Validation set: Average loss: -2.2779
Epoch: 71/1000. Train set: Average loss: -2.2790
Epoch: 71/1000. Validation set: Average loss: -2.2788
Epoch: 72/1000. Train set: Average loss: -2.2785
Epoch: 72/1000. Validation set: Average loss: -2.2773
Epoch: 73/1000. Train set: Average loss: -2.2793
Epoch: 73/1000. Validation set: Average loss: -2.2786
Epoch: 74/1000. Train set: Average loss: -2.2790
Epoch: 74/1000. Validation set: Average loss: -2.2781
Epoch: 75/1000. Train set: Average loss: -2.2786
Epoch: 75/1000. Validation set: Average loss: -2.2785
Epoch: 76/1000. Train set: Average loss: -2.2780
Epoch: 76/1000. Validation set: Average loss: -2.2789
Epoch: 77/1000. Train set: Average loss: -2.2784
Epoch: 77/1000. Validation set: Average loss: -2.2784
Epoch: 78/1000. Train set: Average loss: -2.2788
Epoch: 78/1000. Validation set: Average loss: -2.2789
Epoch: 79/1000. Train set: Average loss: -2.2787
Epoch: 79/1000. Validation set: Average loss: -2.2788
Epoch: 80/1000. Train set: Average loss: -2.2786
44:41<86:49:10, 336.80s/it]  7%|â–‹         | 73/1000 [6:50:17<86:44:00, 336.83s/it]  7%|â–‹         | 74/1000 [6:55:54<86:38:42, 336.85s/it]  8%|â–Š         | 75/1000 [7:01:31<86:32:17, 336.80s/it]  8%|â–Š         | 76/1000 [7:07:08<86:26:27, 336.78s/it]  8%|â–Š         | 77/1000 [7:12:44<86:20:20, 336.75s/it]  8%|â–Š         | 78/1000 [7:18:21<86:15:40, 336.81s/it]  8%|â–Š         | 79/1000 [7:23:58<86:10:17, 336.83s/it]  8%|â–Š         | 80/1000 [7:29:35<86:04:16, 336.80s/it]  8%|â–Š         | 81/1000 [7:35:12<85:58:48, 336.81s/it]  8%|â–Š         | 82/1000 [7:40:49<85:53:47, 336.85s/it]  8%|â–Š         | 83/1000 [7:46:26<85:48:13, 336.85s/it]  8%|â–Š         | 84/1000 [7:52:02<85:41:49, 336.80s/it]  8%|â–Š         | 85/1000 [7:57:39<85:36:17, 336.81s/it]  9%|â–Š         | 86/1000 [8:03:16<85:30:10, 336.77s/it]  9%|â–Š         | 87/1000 [8:08:52<85:22:49, 336.66s/it]  9%|â–‰         | 88/1000 [8:14:28<85:14:52, 336.51s/it]  9%|â–‰         | 89/1000 [8:20:05<85:07:56, 336.42s/it]  9%|â–‰         | 90/1000 [8:25:41<85:00:46, 336.31s/it]  9%|â–‰         | 91/1000 [8:31:17<84:54:04, 336.24s/it]  9%|â–‰         | 92/1000 [8:36:51<84:39:53, 335.68s/it]  9%|â–‰         | 93/1000 [8:42:25<84:25:04, 335.07s/it]  9%|â–‰         | 94/1000 [8:47:58<84:12:01, 334.57s/it] 10%|â–‰         | 95/1000 [8:53:32<84:01:13, 334.22s/it] 10%|â–‰         | 96/1000 [8:59:05<83:51:55, 333.98s/it] 10%|â–‰         | 97/1000 [9:04:38<83:41:54, 333.68s/it] 10%|â–‰         | 98/1000 [9:10:10<83:30:45, 333.31s/it] 10%|â–‰         | 99/1000 [9:15:43<83:23:36, 333.20s/it] 10%|â–ˆ         | 100/1000 [9:21:16<83:15:38, 333.04s/it] 10%|â–ˆ         | 101/1000 [9:26:49<83:08:05, 332.91s/it] 10%|â–ˆ         | 102/1000 [9:32:21<83:01:53, 332.87s/it] 10%|â–ˆ         | 103/1000 [9:37:55<82:58:43, 333.02s/it] 10%|â–ˆ         | 104/1000 [9:43:28<82:56:01, 333.22s/it] 10%|â–ˆ         | 105/1000 [9:49:02<82:51:38, 333.29s/it] 11%|â–ˆ         | 106/1000 [9:54:35<82:46:55, 333.35s/it] 11%|â–ˆ         | 107/1000 [10:00:09<82:41:42, 333.37s/it] 11%|â–ˆ         | 108/1000 [10:05:42<82:36:18, 333.38s/it] 11%|â–ˆ         | 109/1000 [10:11:16<82:31:39, 333.45s/it] 11%|â–ˆ         | 110/1000 [10:16:49<82:26:40, 333.48s/it] 11%|â–ˆ         | 111/1000 [10:22:23<82:21:20, 333.50s/it] 11%|â–ˆ         | 112/1000 [10:27:56<82:15:37, 333.49s/it] 11%|â–ˆâ–        | 113/1000 [10:33:30<82:10:41, 333.53s/it] 11%|â–ˆâ–        | 114/1000 [10:39:04<82:05:47, 333.57s/it] 12%|â–ˆâ–        | 115/1000 [10:44:37<81:59:33, 333.53s/it] 12%|â–ˆâ–        | 116/1000 [10:50:11<81:53:52, 333.52s/it] 12%|â–ˆâ–        | 117/1000 [10:55:44<81:47:54, 333.49s/it] 12%|â–ˆâ–        | 118/1000 [11:01:18<81:42:37, 333.51s/it] 12%|â–ˆâ–        | 119/1000 [11:06:51<81:38:08, 333.58s/it] 12%|â–ˆâ–        | 119/1000 [11:12:25<82:58:13, 339.04s/it]
Epoch: 80/1000. Validation set: Average loss: -2.2788
Epoch: 81/1000. Train set: Average loss: -2.2786
Epoch: 81/1000. Validation set: Average loss: -2.2786
Epoch: 82/1000. Train set: Average loss: -2.2781
Epoch: 82/1000. Validation set: Average loss: -2.2783
Epoch: 83/1000. Train set: Average loss: -2.2782
Epoch: 83/1000. Validation set: Average loss: -2.2780
Epoch: 84/1000. Train set: Average loss: -2.2786
Epoch: 84/1000. Validation set: Average loss: -2.2793
Epoch: 85/1000. Train set: Average loss: -2.2782
Epoch: 85/1000. Validation set: Average loss: -2.2776
Epoch: 86/1000. Train set: Average loss: -2.2778
Epoch: 86/1000. Validation set: Average loss: -2.2786
Epoch: 87/1000. Train set: Average loss: -2.2776
Epoch: 87/1000. Validation set: Average loss: -2.2785
Epoch: 88/1000. Train set: Average loss: -2.2780
Epoch: 88/1000. Validation set: Average loss: -2.2782
Epoch: 89/1000. Train set: Average loss: -2.2785
Epoch: 89/1000. Validation set: Average loss: -2.2781
Epoch: 90/1000. Train set: Average loss: -2.2778
Epoch: 90/1000. Validation set: Average loss: -2.2778
Epoch: 91/1000. Train set: Average loss: -2.2772
Epoch: 91/1000. Validation set: Average loss: -2.2757
Epoch: 92/1000. Train set: Average loss: -2.2781
Epoch: 92/1000. Validation set: Average loss: -2.2787
Epoch: 93/1000. Train set: Average loss: -2.2778
Epoch: 93/1000. Validation set: Average loss: -2.2780
Epoch: 94/1000. Train set: Average loss: -2.2784
Epoch: 94/1000. Validation set: Average loss: -2.2783
Epoch: 95/1000. Train set: Average loss: -2.2778
Epoch: 95/1000. Validation set: Average loss: -2.2775
Epoch: 96/1000. Train set: Average loss: -2.2778
Epoch: 96/1000. Validation set: Average loss: -2.2768
Epoch: 97/1000. Train set: Average loss: -2.2779
Epoch: 97/1000. Validation set: Average loss: -2.2775
Epoch: 98/1000. Train set: Average loss: -2.2784
Epoch: 98/1000. Validation set: Average loss: -2.2777
Epoch: 99/1000. Train set: Average loss: -2.2772
Epoch: 99/1000. Validation set: Average loss: -2.2779
Epoch: 100/1000. Train set: Average loss: -2.2778
Epoch: 100/1000. Validation set: Average loss: -2.2789
Epoch: 101/1000. Train set: Average loss: -2.2782
Epoch: 101/1000. Validation set: Average loss: -2.2781
Epoch: 102/1000. Train set: Average loss: -2.2782
Epoch: 102/1000. Validation set: Average loss: -2.2778
Epoch: 103/1000. Train set: Average loss: -2.2782
Epoch: 103/1000. Validation set: Average loss: -2.2770
Epoch: 104/1000. Train set: Average loss: -2.2785
Epoch: 104/1000. Validation set: Average loss: -2.2784
Epoch: 105/1000. Train set: Average loss: -2.2787
Epoch: 105/1000. Validation set: Average loss: -2.2783
Epoch: 106/1000. Train set: Average loss: -2.2777
Epoch: 106/1000. Validation set: Average loss: -2.2773
Epoch: 107/1000. Train set: Average loss: -2.2787
Epoch: 107/1000. Validation set: Average loss: -2.2784
Epoch: 108/1000. Train set: Average loss: -2.2782
Epoch: 108/1000. Validation set: Average loss: -2.2788
Epoch: 109/1000. Train set: Average loss: -2.2780
Epoch: 109/1000. Validation set: Average loss: -2.2784
Epoch: 110/1000. Train set: Average loss: -2.2773
Epoch: 110/1000. Validation set: Average loss: -2.2772
Epoch: 111/1000. Train set: Average loss: -2.2783
Epoch: 111/1000. Validation set: Average loss: -2.2764
Epoch: 112/1000. Train set: Average loss: -2.2777
Epoch: 112/1000. Validation set: Average loss: -2.2745
Epoch: 113/1000. Train set: Average loss: -2.2759
Epoch: 113/1000. Validation set: Average loss: -2.2750
Epoch: 114/1000. Train set: Average loss: -2.2734
Epoch: 114/1000. Validation set: Average loss: -2.2686
Epoch: 115/1000. Train set: Average loss: -2.2717
Epoch: 115/1000. Validation set: Average loss: -2.2748
Epoch: 116/1000. Train set: Average loss: -2.2748
Epoch: 116/1000. Validation set: Average loss: -2.2717
Epoch: 117/1000. Train set: Average loss: -2.2708
Epoch: 117/1000. Validation set: Average loss: -2.2718
Epoch: 118/1000. Train set: Average loss: -2.2723
Epoch: 118/1000. Validation set: Average loss: -2.2745
Epoch: 119/1000. Train set: Average loss: -2.2754
Epoch: 119/1000. Validation set: Average loss: -2.2756
Epoch: 120/1000. Train set: Average loss: -2.2755
Epoch: 120/1000. Validation set: Average loss: -2.2742
yo?
Training time: 40380.36 seconds
Initial Window: tensor([[-6.6552,  0.5743, -2.0714,  4.9840,  8.8309, -1.0462,  4.6527, 10.7432],
        [-1.2573,  7.1192, 11.5621,  6.1425,  4.6180,  2.1320,  6.4135,  6.0790],
        [ 2.6308,  9.7157, 10.4960, -4.0719,  4.2621,  7.6632,  8.4280,  2.7578],
        [ 3.5314, 11.6621, -4.1574, -0.8401, -0.3725, 13.6223,  2.9777, -2.3032],
        [ 5.1709, 10.8603,  1.5052,  5.3575, 10.2030,  5.3154, -5.2997,  2.4698],
        [10.5231,  9.8095, -0.6279,  6.6502,  6.5805, -5.7486,  2.8205,  3.1672],
        [ 5.0987, -6.5853,  0.9564,  8.1044,  2.5500, -3.7802,  1.2906, 13.2256],
        [-0.7764,  0.9630, -1.0545,  8.2193,  2.7532, -1.9499, -0.7012, 13.0293],
        [ 4.3098, -1.4679,  3.2837,  9.0329, -0.0238, -0.2983,  0.3141, 12.5709],
        [-3.2271,  1.6273,  3.9535, 10.1696,  0.9897,  3.0876,  6.8140, 10.1955]])
Next 10 (9 prediction_length) steps tensor([[-3.2271,  1.6273,  3.9535, 10.1696,  0.9897,  3.0876,  6.8140, 10.1955],
        [-2.4412,  1.4171, 11.8924,  5.5080, -2.5457,  4.1696, 10.3193, -1.5410],
        [ 4.4459,  8.1026,  9.1760, -3.9613,  0.8670,  3.4818, 10.2734, -0.1379],
        [ 5.9674,  8.7886, -4.9083, -0.0667,  0.8987,  9.6319,  9.1128, -0.1576],
        [ 6.9344,  4.8333, -3.8852,  3.3546, 11.8590,  8.3064, -3.2138,  0.3050],
        [ 8.4130,  2.1772, -2.3242,  0.2615, 13.1742, -2.2076,  0.1661, -0.2565],
        [ 8.4880, -1.0374,  0.9322,  1.5652, 12.9802,  4.5080,  0.3643,  4.7071],
        [ 7.4834, -0.7099,  4.0739, 10.1310,  8.0202, -4.6900,  2.4052,  7.3596],
        [ 4.5218, -1.9389,  4.2597, 10.7186, -4.9986,  2.3751,  1.0690, 10.4754],
        [-3.4356,  0.8486,  5.4150, 10.1238,  1.6039,  6.4489, 11.1378,  0.3445]])
predicted output from the model tensor([[[ 1.1528e+02,  1.0782e+03,  7.9935e+02,  1.5302e+02,  1.8903e+02,
           4.6593e+02,  1.5450e+03,  7.3684e+02],
         [-1.3783e+03, -1.7779e+03, -1.0519e+03, -3.5307e+03,  7.9208e+02,
           1.4982e+03,  2.7027e+03,  2.5891e+02],
         [-6.4277e+02, -9.6423e+02, -6.3075e+02, -1.8240e+03,  3.4557e+02,
           8.7059e+02,  1.2346e+03,  2.4382e+01],
         [ 3.1449e+01,  4.5753e+02,  3.8149e+02,  6.1553e+01,  6.7449e+01,
           2.2729e+02,  7.1205e+02,  3.2224e+02],
         [-5.1068e+02, -5.3511e+02, -6.2978e+02, -1.7307e+03,  4.2341e+02,
           8.4161e+02,  1.4972e+03,  2.6940e+02],
         [-1.4712e+02,  3.8374e+03,  4.1079e+03,  2.6834e+03,  9.3986e+02,
           7.0309e+02,  3.9142e+03,  2.5475e+03],
         [-6.4004e+02, -6.6784e+02, -8.0488e+02, -2.1898e+03,  5.3581e+02,
           1.0697e+03,  1.8998e+03,  3.4190e+02],
         [-1.1450e+02,  3.7369e+03,  4.0755e+03,  2.6846e+03,  8.0496e+02,
           6.5140e+02,  3.7557e+03,  2.4590e+03],
         [-5.5884e+01,  3.6660e+02, -8.2367e+01, -8.1027e+02,  2.9301e+02,
           7.6412e+02,  1.4955e+03,  4.5890e+02],
         [ 6.2934e+01,  1.3289e+03,  1.4283e+03,  8.4605e+02,  1.5966e+02,
           2.4171e+02,  1.3380e+03,  8.5592e+02]],

        [[ 8.9273e+01,  9.1119e+02,  6.1924e+02, -3.8276e-01,  1.9079e+02,
           4.7705e+02,  1.4564e+03,  6.4995e+02],
         [-1.3860e+03, -1.8025e+03, -1.0542e+03, -3.5420e+03,  7.8944e+02,
           1.5035e+03,  2.6925e+03,  2.4652e+02],
         [-1.7489e+02, -9.2333e+01, -2.3439e+02, -8.0878e+02,  1.9852e+02,
           5.3739e+02,  9.0670e+02,  1.9022e+02],
         [ 5.6248e+01,  1.6066e+03,  1.7069e+03,  1.0030e+03,  2.5081e+02,
           3.2029e+02,  1.6601e+03,  1.0454e+03],
         [-1.5698e+03, -1.8674e+03, -1.3726e+03, -4.3364e+03,  1.0181e+03,
           1.9115e+03,  3.5179e+03,  4.7918e+02],
         [ 1.0837e+02,  1.0215e+03,  1.0484e+03,  5.8601e+02,  3.5439e+01,
           2.3565e+02,  1.0663e+03,  6.1606e+02],
         [-1.8221e+03, -2.5173e+03, -1.4304e+03, -4.6801e+03,  9.8714e+02,
           2.0295e+03,  3.3786e+03,  1.9595e+02],
         [ 1.8371e+01,  3.5879e+03,  3.9327e+03,  2.5147e+03,  5.8745e+02,
           6.0035e+02,  3.5525e+03,  2.3339e+03],
         [-9.1931e+02, -1.0991e+03, -9.2791e+02, -2.7283e+03,  6.2692e+02,
           1.2687e+03,  2.2001e+03,  3.0211e+02],
         [ 3.5337e+01,  2.9523e+03,  3.2890e+03,  2.1225e+03,  4.2150e+02,
           4.9043e+02,  2.8873e+03,  1.8960e+03]],

        [[ 3.6058e+01,  6.0762e+02,  2.9919e+02, -2.7947e+02,  1.8531e+02,
           5.2945e+02,  1.2903e+03,  4.9193e+02],
         [-1.5739e+02,  4.2018e+02, -1.3514e+02, -1.0796e+03,  4.5863e+02,
           8.7317e+02,  1.9140e+03,  6.3997e+02],
         [-2.5361e+02, -2.5981e+02, -3.0959e+02, -8.5583e+02,  2.0957e+02,
           4.1570e+02,  7.4966e+02,  1.3781e+02],
         [ 8.1950e+01,  8.9135e+02,  9.1630e+02,  5.1366e+02,  4.5386e+01,
           1.9740e+02,  9.3349e+02,  5.4668e+02],
         [-1.9185e+03, -2.5829e+03, -1.5438e+03, -5.0035e+03,  1.0787e+03,
           2.1789e+03,  3.6992e+03,  2.7645e+02],
         [ 4.2718e+01,  3.1540e+03,  3.4788e+03,  2.2142e+03,  4.7826e+02,
           5.3567e+02,  3.1109e+03,  2.0375e+03],
         [-8.4703e+01,  3.9729e+02, -1.2790e+02, -1.0011e+03,  3.5906e+02,
           9.0491e+02,  1.7688e+03,  5.3781e+02],
         [-2.1356e+01,  2.0089e+03,  2.2492e+03,  1.4666e+03,  3.6440e+02,
           3.6343e+02,  2.0047e+03,  1.3024e+03],
         [-5.2402e+02, -5.5267e+02, -7.9611e+02, -2.0353e+03,  5.0856e+02,
           1.0391e+03,  1.7857e+03,  3.1612e+02],
         [-6.8830e+01,  2.6635e+03,  2.9488e+03,  1.9504e+03,  5.3966e+02,
           4.7086e+02,  2.6640e+03,  1.7392e+03]]], device='cuda:0',
       grad_fn=<CatBackward0>) torch.Size([3, 10, 8])
-2.274623587181724
