/scrtp/avon/eb/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Train SR network for lorenz96 model with lr 0.01 using EnergyScorePath scoring rule
  0%|          | 0/1000 [00:00<?, ?it/s]  0%|          | 1/1000 [05:33<92:27:13, 333.17s/it]  0%|          | 2/1000 [11:05<92:15:17, 332.78s/it]  0%|          | 3/1000 [16:38<92:07:35, 332.65s/it]  0%|          | 4/1000 [22:10<92:02:35, 332.69s/it]  0%|          | 5/1000 [27:43<91:56:54, 332.68s/it]  1%|          | 6/1000 [33:16<91:52:51, 332.77s/it]  1%|          | 7/1000 [38:49<91:47:16, 332.77s/it]  1%|          | 8/1000 [44:22<91:42:16, 332.80s/it]  1%|          | 9/1000 [49:53<91:28:08, 332.28s/it]  1%|          | 10/1000 [55:21<91:00:29, 330.94s/it]  1%|          | 11/1000 [1:00:46<90:27:48, 329.29s/it]  1%|          | 12/1000 [1:06:10<89:53:01, 327.51s/it]  1%|▏         | 13/1000 [1:11:33<89:27:54, 326.32s/it]  1%|▏         | 14/1000 [1:16:57<89:07:40, 325.42s/it]  2%|▏         | 15/1000 [1:22:20<88:52:11, 324.80s/it]  2%|▏         | 16/1000 [1:27:44<88:40:25, 324.42s/it]  2%|▏         | 17/1000 [1:33:07<88:30:39, 324.15s/it]  2%|▏         | 18/1000 [1:38:30<88:21:39, 323.93s/it]  2%|▏         | 19/1000 [1:43:54<88:14:17, 323.81s/it]  2%|▏         | 20/1000 [1:49:18<88:07:52, 323.75s/it]  2%|▏         | 21/1000 [1:54:41<88:01:50, 323.71s/it]  2%|▏         | 22/1000 [2:00:05<87:55:17, 323.64s/it]  2%|▏         | 23/1000 [2:05:28<87:49:26, 323.61s/it]  2%|▏         | 24/1000 [2:10:53<87:49:37, 323.95s/it]  2%|▎         | 25/1000 [2:16:17<87:46:42, 324.10s/it]  3%|▎         | 26/1000 [2:21:42<87:44:12, 324.28s/it]  3%|▎         | 27/1000 [2:27:07<87:39:32, 324.33s/it]  3%|▎         | 28/1000 [2:32:31<87:35:57, 324.44s/it]  3%|▎         | 29/1000 [2:37:56<87:31:37, 324.51s/it]  3%|▎         | 30/1000 [2:43:21<87:27:04, 324.56s/it]  3%|▎         | 31/1000 [2:48:45<87:22:07, 324.59s/it]  3%|▎         | 32/1000 [2:54:10<87:16:58, 324.61s/it]  3%|▎         | 33/1000 [2:59:35<87:12:32, 324.67s/it]  3%|▎         | 34/1000 [3:04:59<87:05:56, 324.59s/it]  4%|▎         | 35/1000 [3:10:24<87:02:36, 324.72s/it]  4%|▎         | 36/1000 [3:15:49<86:59:22, 324.86s/it]  4%|▎         | 37/1000 [3:21:14<86:53:59, 324.86s/it]  4%|▍         | 38/1000 [3:26:39<86:49:13, 324.90s/it]  4%|▍         | 39/1000 [3:32:05<86:45:55, 325.03s/it]  4%|▍         | 40/1000 [3:37:29<86:39:24, 324.96s/it]  4%|▍         | 41/1000 [3:42:53<86:28:49, 324.64s/it]  4%|▍         | 42/1000 [3:48:17<86:20:10, 324.44s/it]  4%|▍         | 43/1000 [3:53:42<86:17:25, 324.60s/it]  4%|▍         | 44/1000 [3:59:07<86:14:08, 324.74s/it]  4%|▍         | 45/1000 [4:04:32<86:09:04, 324.76s/it]  5%|▍         | 46/1000 [4:09:57<86:04:21, 324.80s/it]  5%|▍         | 47/1000 [4:15:22<85:59:55, 324.86s/it]  5%|▍         | 48/1000 [4:20:47<85:55:11, 324.91s/it]  5%|▍         | 49/1000 [4:26:12<85:50:52, 324.98s/it]  5%|▌         | 50/1000 [4:31:37<85:45:00, 324.95s/it]  5%|▌         | 51/1000 [4:37:02<85:40:27, 325.00s/it]  5%|▌         | 52/1000 [4:42:27<85:36:15, 325.08s/it]  5%|▌         | 53/1000 [4:47:53<85:31:54, 325.15s/it]  5%|▌         | 54/1000 [4:53:18<85:26:33, 325.15s/it]  6%|▌         | 55/1000 [4:58:43<85:20:44, 325.13s/it]  6%|▌         | 56/1000 [5:04:08<85:15:05, 325.11s/it]  6%|▌         | 57/1000 [5:09:33<85:10:16, 325.15s/it]  6%|▌         | 58/1000 [5:14:58<85:05:14, 325.17s/it]  6%|▌         | 59/1000 [5:20:24<85:00:24, 325.21s/it]  6%|▌         | 60/1000 [5:25:49<84:54:13, 325.16s/it]  6%|▌         | 61/1000 [5:31:14<84:47:27, 325.08s/it]  6%|▌         | 62/1000 [5:36:39<84:42:58, 325.14s/it]  6%|▋         | 63/1000 [5:42:04<84:37:35, 325.14s/it]  6%|▋         | 64/1000 [5:47:29<84:32:08, 325.14s/it]  6%|▋         | 65/1000 [5:52:55<84:27:19, 325.18s/it]  7%|▋         | 66/1000 [5:58:20<84:21:48, 325.17s/it]  7%|▋         | 67/1000 [6:03:45<84:17:01, 325.21s/it]  7%|▋         | 68/1000 [6:09:10<84:11:02, 325.17s/it]  7%|▋         | 69/1000 [6:14:35<84:04:54, 325.13s/it]  7%|▋         | 70/1000 [6:20:00<83:56:11, 324.92s/it]  7%|▋         | 71/1000 [6:25:24<83:46:36, 3Epoch: 1/1000. Train set: Average loss: 31.2971
Epoch: 1/1000. Validation set: Average loss: 27.5826
Epoch: 2/1000. Train set: Average loss: 25.8445
Epoch: 2/1000. Validation set: Average loss: 23.4134
Epoch: 3/1000. Train set: Average loss: 23.0007
Epoch: 3/1000. Validation set: Average loss: 22.6430
Epoch: 4/1000. Train set: Average loss: 22.4270
Epoch: 4/1000. Validation set: Average loss: 22.1854
Epoch: 5/1000. Train set: Average loss: 21.8557
Epoch: 5/1000. Validation set: Average loss: 21.5599
Epoch: 6/1000. Train set: Average loss: 21.1492
Epoch: 6/1000. Validation set: Average loss: 20.4539
Epoch: 7/1000. Train set: Average loss: 20.2600
Epoch: 7/1000. Validation set: Average loss: 19.7007
Epoch: 8/1000. Train set: Average loss: 19.7009
Epoch: 8/1000. Validation set: Average loss: 19.2252
Epoch: 9/1000. Train set: Average loss: 18.9289
Epoch: 9/1000. Validation set: Average loss: 18.6495
Epoch: 10/1000. Train set: Average loss: 17.9621
Epoch: 10/1000. Validation set: Average loss: 19.3364
Epoch: 11/1000. Train set: Average loss: 18.5180
Epoch: 11/1000. Validation set: Average loss: 18.2295
Epoch: 12/1000. Train set: Average loss: 17.9886
Epoch: 12/1000. Validation set: Average loss: 17.5215
Epoch: 13/1000. Train set: Average loss: 17.4655
Epoch: 13/1000. Validation set: Average loss: 17.5019
Epoch: 14/1000. Train set: Average loss: 17.2671
Epoch: 14/1000. Validation set: Average loss: 17.6635
Epoch: 15/1000. Train set: Average loss: 17.2727
Epoch: 15/1000. Validation set: Average loss: 17.3680
Epoch: 16/1000. Train set: Average loss: 17.1818
Epoch: 16/1000. Validation set: Average loss: 17.0411
Epoch: 17/1000. Train set: Average loss: 17.0231
Epoch: 17/1000. Validation set: Average loss: 16.7242
Epoch: 18/1000. Train set: Average loss: 16.8832
Epoch: 18/1000. Validation set: Average loss: 16.8858
Epoch: 19/1000. Train set: Average loss: 16.7916
Epoch: 19/1000. Validation set: Average loss: 16.6488
Epoch: 20/1000. Train set: Average loss: 16.7952
Epoch: 20/1000. Validation set: Average loss: 16.5384
Epoch: 21/1000. Train set: Average loss: 16.7018
Epoch: 21/1000. Validation set: Average loss: 16.6490
Epoch: 22/1000. Train set: Average loss: 16.7484
Epoch: 22/1000. Validation set: Average loss: 16.7778
Epoch: 23/1000. Train set: Average loss: 16.7804
Epoch: 23/1000. Validation set: Average loss: 16.8871
Epoch: 24/1000. Train set: Average loss: 16.9454
Epoch: 24/1000. Validation set: Average loss: 16.6079
Epoch: 25/1000. Train set: Average loss: 16.6661
Epoch: 25/1000. Validation set: Average loss: 16.5977
Epoch: 26/1000. Train set: Average loss: 16.7513
Epoch: 26/1000. Validation set: Average loss: 16.6528
Epoch: 27/1000. Train set: Average loss: 16.6958
Epoch: 27/1000. Validation set: Average loss: 16.6576
Epoch: 28/1000. Train set: Average loss: 16.5882
Epoch: 28/1000. Validation set: Average loss: 16.5411
Epoch: 29/1000. Train set: Average loss: 16.5406
Epoch: 29/1000. Validation set: Average loss: 16.4883
Epoch: 30/1000. Train set: Average loss: 16.5326
Epoch: 30/1000. Validation set: Average loss: 16.5883
Epoch: 31/1000. Train set: Average loss: 16.7695
Epoch: 31/1000. Validation set: Average loss: 17.1115
Epoch: 32/1000. Train set: Average loss: 17.4827
Epoch: 32/1000. Validation set: Average loss: 17.7785
Epoch: 33/1000. Train set: Average loss: 16.8922
Epoch: 33/1000. Validation set: Average loss: 16.8184
Epoch: 34/1000. Train set: Average loss: 16.8758
Epoch: 34/1000. Validation set: Average loss: 16.8825
Epoch: 35/1000. Train set: Average loss: 17.1390
Epoch: 35/1000. Validation set: Average loss: 16.8325
Epoch: 36/1000. Train set: Average loss: 17.0826
Epoch: 36/1000. Validation set: Average loss: 16.7449
Epoch: 37/1000. Train set: Average loss: 16.6729
Epoch: 37/1000. Validation set: Average loss: 16.6636
Epoch: 38/1000. Train set: Average loss: 16.6062
Epoch: 38/1000. Validation set: Average loss: 16.7374
Epoch: 39/1000. Train set: Average loss: 16.8453
Epoch: 39/1000. Validation set: Average loss: 17.1214
Epoch: 40/1000. Train set: Average loss: 17.0059
Epoch: 40/1000. Validation set: Average loss: 16.9055
Epoch: 41/1000. Train set: Average loss: 16.5821
Epoch: 41/1000. Validation set: Average loss: 16.4193
Epoch: 42/1000. Train set: Average loss: 16.4703
Epoch: 42/1000. Validation set: Average loss: 16.7095
Epoch: 43/1000. Train set: Average loss: 16.5503
Epoch: 43/1000. Validation set: Average loss: 16.7637
Epoch: 44/1000. Train set: Average loss: 16.4759
Epoch: 44/1000. Validation set: Average loss: 16.4628
Epoch: 45/1000. Train set: Average loss: 16.5434
Epoch: 45/1000. Validation set: Average loss: 16.6884
Epoch: 46/1000. Train set: Average loss: 16.5357
Epoch: 46/1000. Validation set: Average loss: 16.7254
Epoch: 47/1000. Train set: Average loss: 16.3567
Epoch: 47/1000. Validation set: Average loss: 16.4511
Epoch: 48/1000. Train set: Average loss: 16.4662
Epoch: 48/1000. Validation set: Average loss: 16.3971
Epoch: 49/1000. Train set: Average loss: 16.4741
Epoch: 49/1000. Validation set: Average loss: 16.3903
Epoch: 50/1000. Train set: Average loss: 16.4516
Epoch: 50/1000. Validation set: Average loss: 16.3920
Epoch: 51/1000. Train set: Average loss: 16.3509
Epoch: 51/1000. Validation set: Average loss: 16.6897
Epoch: 52/1000. Train set: Average loss: 16.3647
Epoch: 52/1000. Validation set: Average loss: 16.5312
Epoch: 53/1000. Train set: Average loss: 16.3616
Epoch: 53/1000. Validation set: Average loss: 16.3640
Epoch: 54/1000. Train set: Average loss: 16.3475
Epoch: 54/1000. Validation set: Average loss: 16.2757
Epoch: 55/1000. Train set: Average loss: 16.2482
Epoch: 55/1000. Validation set: Average loss: 16.2975
Epoch: 56/1000. Train set: Average loss: 16.3045
Epoch: 56/1000. Validation set: Average loss: 16.4131
Epoch: 57/1000. Train set: Average loss: 16.2954
Epoch: 57/1000. Validation set: Average loss: 16.5577
Epoch: 58/1000. Train set: Average loss: 16.5836
Epoch: 58/1000. Validation set: Average loss: 16.4928
Epoch: 59/1000. Train set: Average loss: 16.4891
Epoch: 59/1000. Validation set: Average loss: 16.2345
Epoch: 60/1000. Train set: Average loss: 16.3977
Epoch: 60/1000. Validation set: Average loss: 16.4612
Epoch: 61/1000. Train set: Average loss: 16.2872
Epoch: 61/1000. Validation set: Average loss: 16.2653
Epoch: 62/1000. Train set: Average loss: 16.3411
Epoch: 62/1000. Validation set: Average loss: 16.5339
Epoch: 63/1000. Train set: Average loss: 16.4952
Epoch: 63/1000. Validation set: Average loss: 16.4531
Epoch: 64/1000. Train set: Average loss: 16.3752
Epoch: 64/1000. Validation set: Average loss: 16.4198
Epoch: 65/1000. Train set: Average loss: 16.3307
Epoch: 65/1000. Validation set: Average loss: 16.2275
Epoch: 66/1000. Train set: Average loss: 16.2369
Epoch: 66/1000. Validation set: Average loss: 16.2658
Epoch: 67/1000. Train set: Average loss: 16.5369
Epoch: 67/1000. Validation set: Average loss: 16.5455
Epoch: 68/1000. Train set: Average loss: 16.7363
Epoch: 68/1000. Validation set: Average loss: 16.6054
Epoch: 69/1000. Train set: Average loss: 16.3571
Epoch: 69/1000. Validation set: Average loss: 16.6643
Epoch: 70/1000. Train set: Average loss: 16.5594
Epoch: 70/1000. Validation set: Average loss: 16.5631
Epoch: 71/1000. Train set: Average loss: 16.4553
Epoch: 71/1000. Validation set: Average loss: 16.6920
Epoch: 72/1000. Train set: Average loss: 17.0950
Epoch: 72/1000. Validation set: Average loss: 20.0883
Epoch: 73/1000. Train set: Average loss: 18.3248
Epoch: 73/1000. Validation set: Average loss: 17.5583
Epoch: 74/1000. Train set: Average loss: 17.2084
Epoch: 74/1000. Validation set: Average loss: 17.8833
Epoch: 75/1000. Train set: Average loss: 18.5153
Epoch: 75/1000. Validation set: Average loss: 18.8163
Epoch: 76/1000. Train set: Average loss: 18.4584
Epoch: 76/1000. Validation set: Average loss: 18.4483
Epoch: 77/1000. Train set: Average loss: 18.0888
Epoch: 77/1000. Validation set: Average loss: 18.6639
Epoch: 78/1000. Train set: Average loss: 17.9619
Epoch: 78/1000. Validation set: Average loss: 17.4141
Epoch: 79/1000. Train set: Average loss: 17.0803
Epoch: 79/1000. Validation set: Average loss: 16.8944
Epoch: 80/1000. Train set: Average loss: 16.7788
24.65s/it]  7%|▋         | 72/1000 [6:30:48<83:39:44, 324.55s/it]  7%|▋         | 73/1000 [6:36:12<83:32:53, 324.46s/it]  7%|▋         | 74/1000 [6:41:36<83:26:29, 324.39s/it]  8%|▊         | 75/1000 [6:47:01<83:21:16, 324.41s/it]  8%|▊         | 76/1000 [6:52:25<83:15:30, 324.38s/it]  8%|▊         | 77/1000 [6:57:49<83:09:59, 324.38s/it]  8%|▊         | 78/1000 [7:03:15<83:09:04, 324.67s/it]  8%|▊         | 79/1000 [7:08:40<83:06:55, 324.88s/it]  8%|▊         | 80/1000 [7:14:04<82:57:07, 324.60s/it]  8%|▊         | 81/1000 [7:19:28<82:48:34, 324.39s/it]  8%|▊         | 82/1000 [7:24:52<82:41:12, 324.26s/it]  8%|▊         | 83/1000 [7:30:16<82:33:38, 324.12s/it]  8%|▊         | 84/1000 [7:35:39<82:26:18, 323.99s/it]  8%|▊         | 85/1000 [7:41:03<82:19:10, 323.88s/it]  9%|▊         | 86/1000 [7:46:27<82:13:24, 323.86s/it]  9%|▊         | 87/1000 [7:51:51<82:07:25, 323.82s/it]  9%|▉         | 88/1000 [7:57:14<82:01:48, 323.80s/it]  9%|▉         | 89/1000 [8:02:38<81:55:59, 323.77s/it]  9%|▉         | 90/1000 [8:08:02<81:52:36, 323.91s/it]  9%|▉         | 91/1000 [8:13:27<81:48:43, 324.01s/it]  9%|▉         | 92/1000 [8:18:51<81:44:25, 324.08s/it]  9%|▉         | 93/1000 [8:24:15<81:39:38, 324.12s/it]  9%|▉         | 94/1000 [8:29:39<81:34:09, 324.12s/it] 10%|▉         | 95/1000 [8:35:03<81:27:59, 324.07s/it] 10%|▉         | 96/1000 [8:40:27<81:22:10, 324.04s/it] 10%|▉         | 97/1000 [8:45:51<81:17:05, 324.06s/it] 10%|▉         | 98/1000 [8:51:15<81:11:55, 324.08s/it] 10%|▉         | 99/1000 [8:56:39<81:06:05, 324.05s/it] 10%|▉         | 99/1000 [9:02:02<82:13:07, 328.51s/it]
Epoch: 80/1000. Validation set: Average loss: 16.8995
Epoch: 81/1000. Train set: Average loss: 16.7984
Epoch: 81/1000. Validation set: Average loss: 16.7974
Epoch: 82/1000. Train set: Average loss: 16.6850
Epoch: 82/1000. Validation set: Average loss: 16.6774
Epoch: 83/1000. Train set: Average loss: 16.6596
Epoch: 83/1000. Validation set: Average loss: 16.9178
Epoch: 84/1000. Train set: Average loss: 16.7801
Epoch: 84/1000. Validation set: Average loss: 16.7932
Epoch: 85/1000. Train set: Average loss: 16.8871
Epoch: 85/1000. Validation set: Average loss: 17.1820
Epoch: 86/1000. Train set: Average loss: 17.2032
Epoch: 86/1000. Validation set: Average loss: 17.3869
Epoch: 87/1000. Train set: Average loss: 17.5125
Epoch: 87/1000. Validation set: Average loss: 16.8803
Epoch: 88/1000. Train set: Average loss: 17.0420
Epoch: 88/1000. Validation set: Average loss: 17.2472
Epoch: 89/1000. Train set: Average loss: 17.3667
Epoch: 89/1000. Validation set: Average loss: 17.0925
Epoch: 90/1000. Train set: Average loss: 17.2119
Epoch: 90/1000. Validation set: Average loss: 16.8147
Epoch: 91/1000. Train set: Average loss: 16.6940
Epoch: 91/1000. Validation set: Average loss: 16.6825
Epoch: 92/1000. Train set: Average loss: 16.6599
Epoch: 92/1000. Validation set: Average loss: 16.5017
Epoch: 93/1000. Train set: Average loss: 16.5339
Epoch: 93/1000. Validation set: Average loss: 16.4828
Epoch: 94/1000. Train set: Average loss: 16.5011
Epoch: 94/1000. Validation set: Average loss: 16.5888
Epoch: 95/1000. Train set: Average loss: 16.5861
Epoch: 95/1000. Validation set: Average loss: 16.6863
Epoch: 96/1000. Train set: Average loss: 16.7007
Epoch: 96/1000. Validation set: Average loss: 16.8027
Epoch: 97/1000. Train set: Average loss: 16.6243
Epoch: 97/1000. Validation set: Average loss: 16.4900
Epoch: 98/1000. Train set: Average loss: 16.4840
Epoch: 98/1000. Validation set: Average loss: 16.5937
Epoch: 99/1000. Train set: Average loss: 16.6873
Epoch: 99/1000. Validation set: Average loss: 16.8564
Epoch: 100/1000. Train set: Average loss: 18.1737
Epoch: 100/1000. Validation set: Average loss: 17.8133
yo?
Training time: 32556.96 seconds
Initial Window: tensor([[-6.6552,  0.5743, -2.0714,  4.9840,  8.8309, -1.0462,  4.6527, 10.7432],
        [-1.2573,  7.1192, 11.5621,  6.1425,  4.6180,  2.1320,  6.4135,  6.0790],
        [ 2.6308,  9.7157, 10.4960, -4.0719,  4.2621,  7.6632,  8.4280,  2.7578],
        [ 3.5314, 11.6621, -4.1574, -0.8401, -0.3725, 13.6223,  2.9777, -2.3032],
        [ 5.1709, 10.8603,  1.5052,  5.3575, 10.2030,  5.3154, -5.2997,  2.4698],
        [10.5231,  9.8095, -0.6279,  6.6502,  6.5805, -5.7486,  2.8205,  3.1672],
        [ 5.0987, -6.5853,  0.9564,  8.1044,  2.5500, -3.7802,  1.2906, 13.2256],
        [-0.7764,  0.9630, -1.0545,  8.2193,  2.7532, -1.9499, -0.7012, 13.0293],
        [ 4.3098, -1.4679,  3.2837,  9.0329, -0.0238, -0.2983,  0.3141, 12.5709],
        [-3.2271,  1.6273,  3.9535, 10.1696,  0.9897,  3.0876,  6.8140, 10.1955]])
Next 10 (9 prediction_length) steps tensor([[-3.2271,  1.6273,  3.9535, 10.1696,  0.9897,  3.0876,  6.8140, 10.1955],
        [-2.4412,  1.4171, 11.8924,  5.5080, -2.5457,  4.1696, 10.3193, -1.5410],
        [ 4.4459,  8.1026,  9.1760, -3.9613,  0.8670,  3.4818, 10.2734, -0.1379],
        [ 5.9674,  8.7886, -4.9083, -0.0667,  0.8987,  9.6319,  9.1128, -0.1576],
        [ 6.9344,  4.8333, -3.8852,  3.3546, 11.8590,  8.3064, -3.2138,  0.3050],
        [ 8.4130,  2.1772, -2.3242,  0.2615, 13.1742, -2.2076,  0.1661, -0.2565],
        [ 8.4880, -1.0374,  0.9322,  1.5652, 12.9802,  4.5080,  0.3643,  4.7071],
        [ 7.4834, -0.7099,  4.0739, 10.1310,  8.0202, -4.6900,  2.4052,  7.3596],
        [ 4.5218, -1.9389,  4.2597, 10.7186, -4.9986,  2.3751,  1.0690, 10.4754],
        [-3.4356,  0.8486,  5.4150, 10.1238,  1.6039,  6.4489, 11.1378,  0.3445]])
predicted output from the model tensor([[[ 1.5293e+00,  2.8724e+00,  8.2580e+00, -2.1992e-01, -1.9471e-01,
           1.9028e+00,  7.3398e+00,  9.0695e-01],
         [ 1.6559e+00,  8.8313e+00,  5.9007e+00, -1.2073e+00,  4.4426e+00,
           1.0714e+01,  6.8222e+00, -1.9720e+00],
         [-6.7996e-01,  1.6994e+00,  7.8259e+00,  1.3327e+00,  1.2303e+00,
           3.0159e+00,  8.2109e+00,  2.9905e-01],
         [ 4.9378e-01,  3.1282e+00,  6.5163e+00, -8.9621e-01,  2.3249e+00,
           4.5637e+00,  7.1188e+00, -1.3977e+00],
         [ 1.8922e+00,  9.8031e+00, -3.0838e-01, -2.0603e-01,  3.1533e+00,
           1.0735e+01,  8.7238e-02, -1.3159e-01],
         [ 2.2214e+00,  5.2969e-01,  6.3108e-01,  3.9814e-01,  2.1663e+00,
           5.4133e-01,  3.6343e-01,  8.0010e-01],
         [-6.3346e-01, -7.3513e-01,  1.7681e+00,  1.5774e+01, -1.0128e+00,
          -2.4453e+00,  2.4481e+00,  1.8349e+01],
         [-5.0988e+00,  2.7186e+00,  1.1952e+01,  1.3065e+01, -1.0895e+00,
           4.1231e+00,  1.2244e+01,  1.0729e+01],
         [ 1.7432e+00,  2.9086e+00,  8.0089e+00, -2.5264e-01, -4.6133e-01,
           1.7157e+00,  6.9253e+00,  1.1185e+00],
         [ 2.1820e+00,  1.2431e+01,  2.0092e+00, -5.9460e-01,  4.6423e+00,
           1.3711e+01,  2.8243e+00, -1.1361e+00]],

        [[ 6.9816e-01,  2.9106e+00,  9.5245e+00,  7.9232e-01,  1.9625e-01,
           2.6634e+00,  8.9780e+00,  1.0960e+00],
         [ 1.3032e+00,  7.2755e+00,  8.9115e+00, -1.5989e+00,  4.4482e+00,
           9.5064e+00,  9.8961e+00, -2.5532e+00],
         [ 1.5505e+00,  8.0989e+00,  5.3888e-02, -3.2850e-01,  2.5939e+00,
           8.6781e+00,  3.2923e-01, -1.9897e-01],
         [ 5.1760e+00,  5.5222e+00, -9.1686e-01,  3.5212e+00,  7.0906e+00,
           8.4726e+00, -8.4153e-01,  2.8256e+00],
         [ 8.2087e+00, -2.6197e+00,  2.2613e+00,  1.3495e+00,  9.6258e+00,
          -2.0099e+00,  1.9549e+00,  1.3045e+00],
         [-2.8353e+00, -2.2139e-01,  4.0911e+00,  1.7977e+01, -1.3221e+00,
          -1.4131e+00,  4.5220e+00,  1.8832e+01],
         [-5.9970e+00,  3.2966e+00,  1.5952e+01,  1.3052e+01,  6.8720e-02,
           6.1520e+00,  1.6265e+01,  8.8780e+00],
         [ 5.4288e-01,  2.0080e+00,  8.6736e+00, -5.6624e-01,  1.7263e+00,
           3.2167e+00,  8.6756e+00, -8.9681e-01],
         [-4.5733e-01,  3.5320e+00,  1.6115e+01,  2.5732e-01,  2.8107e+00,
           5.8246e+00,  1.6941e+01, -1.6347e+00],
         [ 1.7009e+00,  8.9407e+00,  4.9148e+00, -8.9715e-01,  4.2079e+00,
           1.0599e+01,  5.6687e+00, -1.5498e+00]],

        [[-4.7654e-01,  2.8384e+00,  1.1619e+01,  2.1115e+00,  9.7382e-01,
           4.0252e+00,  1.1604e+01,  1.0872e+00],
         [ 3.2013e+00,  2.2220e+01, -3.7735e+00,  1.4941e+00,  7.7756e+00,
           2.7017e+01, -1.3569e+00,  1.8796e-01],
         [ 1.9978e+00, -8.4817e-01,  1.2292e+00,  1.1568e+00,  2.3282e+00,
          -8.7359e-01,  1.0330e+00,  1.7276e+00],
         [ 1.8222e+00, -1.9750e+00,  1.7928e+00,  1.2250e+01,  2.3336e+00,
          -2.8571e+00,  2.3166e+00,  1.4468e+01],
         [-3.6009e+00,  1.6265e+00,  8.2223e+00,  1.2366e+01, -9.0807e-01,
           2.0329e+00,  8.5383e+00,  1.1412e+01],
         [ 2.1641e-01,  2.4445e+00,  8.6084e+00,  1.1527e+00,  3.7232e-01,
           2.6248e+00,  8.2899e+00,  1.0550e+00],
         [ 2.2290e+00,  1.2614e+01,  2.6411e+00, -7.3101e-01,  4.8650e+00,
           1.4032e+01,  3.5024e+00, -1.3461e+00],
         [-1.1529e+00,  9.9357e-01,  4.2742e+00,  3.4197e+00,  1.4326e-02,
           1.5889e+00,  4.2326e+00,  2.9758e+00],
         [ 3.4356e-01,  1.6032e+00,  5.0465e+00, -5.3622e-01,  1.3164e+00,
           2.4805e+00,  5.2158e+00, -6.7890e-01],
         [ 3.2481e-01,  1.9673e+00,  8.3402e+00, -7.4165e-01,  1.9604e+00,
           3.3229e+00,  8.6691e+00, -1.3109e+00]]], device='cuda:0',
       grad_fn=<CatBackward0>) torch.Size([3, 10, 8])
val_loss_list [27.582633551210165, 23.41336151957512, 22.642982106655836, 22.1853544479236, 21.55993221933022, 20.453866968862712, 19.700651405844837, 19.225185233168304, 18.64953055093065, 19.33639382524416, 18.2295387648046, 17.52148353913799, 17.501877916045487, 17.663462290074676, 17.368042249931023, 17.04111647931859, 16.724196093389764, 16.88576165563427, 16.648824548814446, 16.538374266587198, 16.648989292560145, 16.77783306967467, 16.887115220539272, 16.60790128237568, 16.597666940419003, 16.65276925568469, 16.657649936154485, 16.541095576714724, 16.488329659216106, 16.588251751614735, 17.111451285192743, 17.778526494279504, 16.81843143189326, 16.882529384223744, 16.832475528353825, 16.74485957948491, 16.66362692019902, 16.737362400395796, 17.12139836931601, 16.905532649485394, 16.4192807036452, 16.709527803119272, 16.763736446388066, 16.462794680148363, 16.688399752834812, 16.725445427699015, 16.45109168626368, 16.397126393858343, 16.390335796633735, 16.392038644989952, 16.689743676921353, 16.53120214212686, 16.36402131919749, 16.27566766506061, 16.297533684875816, 16.41314635425806, 16.55766197037883, 16.49276079935953, 16.23446561442688, 16.461155099328607, 16.265261020278558, 16.53392610931769, 16.453149901004508, 16.419842495350167, 16.227545245317742, 16.26576153631322, 16.545459581771865, 16.605358388274908, 16.664309739600867, 16.563129234826192, 16.69204523600638, 20.088306240737438, 17.558333258144557, 17.88332142913714, 18.81632252689451, 18.448264150880277, 18.663931590970606, 17.4140974546317, 16.894389687571675, 16.899517056997865, 16.797442824579775, 16.6774030898232, 16.917771216714755, 16.793238549726084, 17.181987350340933, 17.38694879063405, 16.88029628433287, 17.247213233727962, 17.0924683525227, 16.814706299221143, 16.682541381567717, 16.501661349320784, 16.482793556526303, 16.5888385088183, 16.68625639588572, 16.802652389742434, 16.489966965746135, 16.593661531805992, 16.85637122229673, 17.81332943821326]
17.66376058757305
